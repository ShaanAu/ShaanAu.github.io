[{"authors":["admin"],"categories":null,"content":"I’m a Data Science Degree Apprentice at Santander UK where I rotate across various teams within the corporate and retail bank whilst working as a Data Scientist. All whilst studying for a degree withn Data Science.\nI\u0026rsquo;ve worked on a variety of projects ranging from chatbots, mortgage churn and entity resolotion using a wide range of technologies but primarily Python and SQL.\n","date":1606089600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1606089600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://shaanaucharagram.com/author/shaan-aucharagram/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shaan-aucharagram/","section":"authors","summary":"I’m a Data Science Degree Apprentice at Santander UK where I rotate across various teams within the corporate and retail bank whilst working as a Data Scientist. All whilst studying for a degree withn Data Science.","tags":null,"title":"Shaan Aucharagram","type":"authors"},{"authors":["Shaan Aucharagram"],"categories":["NLP"],"content":"In this tutorial, we will be retrieving random cat facts using a Web API with the requests package in python.\nNext, we will use NLP with spaCy to tokenise, remove stop words and apply lemmatization on our text data.\nFinally will use the cleaned data and then perform sentiment analysis using the textblob package to create a score for each fact and to see whether the score is:\n Positive: Score \u0026gt; 0 Negative: Score \u0026lt; 0 Neutral: Score = 0  GitHub\nThe notebook is available on my GitHub. I would suggest cloning the repository for anyone looking to work on the code themselves.\nRequired Packages\nIf the package is failing to import, you can resolve the issue using pip e.g.\n!pip3 install pandas  # Import Packages and Libraries import json import requests import pandas as pd import numpy as np from textblob import TextBlob import spacy from spacy import displacy from spacy.matcher import Matcher from urllib.request import urlopen import plotly.express as px import matplotlib.pyplot as plt import itertools from wordcloud import WordCloud from collections import Counter import cufflinks as cf  Retrieve Random Cat Facts\nThe API I am using to retrieve the random cat facts, automatically generates a random fact each time you call the specified URL. Each time it will append the results to responses.\ndef retrieve_cat_facts(url, no_facts): ''' Retrieve Random Cat Facts Using API call Append Into Responses ''' responses = list() for i in range(0,no_facts): response = requests.get(f\u0026quot;{url}\u0026quot;) data=json.loads(response.text) responses.append(data) return responses, response responses, response = retrieve_cat_facts(url = 'https://cat-fact.herokuapp.com/facts/random', no_facts = 20)  Output\nThe function will append all the responses from the function into dictionaries within a list. This is just a snapshot of the first dictionary within the list. You can use the following code below to check that the API call is succesful.\nif response.status_code == 200: print('Successful API call')  Load Pre-trained Model\nnlp = spacy.load('en')  Create Lists of Lists of \u0026lsquo;Text\u0026rsquo; Data\nI decided to create a list of lists of all the random cat facts, as we are only concerned with the text data.\ndef create_list_of_lists(number_of_facts): ''' Create A List Of Lists for the 'text' columns ''' doc_responses = list() for x in range (0, number_of_facts): doc = nlp(responses[x]['text']) doc_responses.append(doc) return doc_responses doc_responses = create_list_of_lists(20)  Length of Sentences\nThe following code will loop through the words of each sentence, before outputting the number of words in a sentence.\ndef getLengthSentences(number_of_facts, responses): ''' Get Length of Sentence for each random cat fact ''' for z in range (0, number_of_facts): #passing text into nlp object sentence = nlp(responses[z]['text']) #Identify the sentences using attribute sentences = list(sentence.sents) # Reading the sentences for sent in sentences: print('Sentence: ', sent) print(\u0026quot;The length of the sentences:\u0026quot;, len(sent)) getLengthSentences(20, responses)  Stop Words\nStop words are the most common words in any NLP model. To analyze text data and build NLP models, these stopwords might not add much value to the meaning of the document. Generally, the most common words used in a text are\n “the” “is” “in” \u0026ldquo;a\u0026rdquo;  We will remove the stopwords from our text data before scoring our random cat facts.\nstopwords = spacy.lang.en.stop_words.STOP_WORDS # check the length of Stop Words print(\u0026quot;The length of stopwords:\u0026quot;, len(stopwords)) for i in list(stopwords)[:20]: print(i)  def remove_stopwords(doc): '''Remove Stop Words From Text Data''' final_doc = [] for sentence in doc: print(\u0026quot;Number of tokens in the doc:\u0026quot;, len(sentence)) element = [] for word in sentence: if not word.is_stop: element.append(word) final_doc.append(element) print(\u0026quot;Number of tokens after removing stopwords:\u0026quot;, len(element)) return final_doc doc_no_stopwords = remove_stopwords(doc_responses)  Remove Punctuation\nGenerally, you want to remove punctuation from text data, for this project we will remove punctuation. However, there are occasions where punctuation can be used to gain insight into text data.\ndef removePunctuation(doc): ''' Remove Punctuation From Text Data ''' final_doc = [] for sentence in doc: element = [] for word in sentence: if not word.is_punct: element.append(word) final_doc.append(element) return final_doc doc_no_punctuation = removePunctuation(final_doc)  Lemmatization\nIn simpler terms, a method that switches any kind of a word to its base root mode is called Lemmatization.\n‘troubled’ -\u0026gt; Lemmatization -\u0026gt; ‘trouble’\n‘neglected’ -\u0026gt; Lemmatization -\u0026gt; ‘neglect’\ndef lemmatization(doc): ''' Apply Lemmatization On Text Data ''' final_doc = [] for sentence in doc: element = [] for word in sentence: element.append(word.lemma_) final_doc.append(element) return final_doc doc_lemmatization = lemmatization(doc_no_punctuation)  Comparison\ndoc_no_punctuation[0], doc_no_stopwords[0], doc_lemmatization[0]  From the below image you can see that each of our functions has worked as expected. Part of Speech Tagging\nThe process of classifying words into their parts of speech and labelling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging.\nThis can be useful within text data to gain a greater overview of what our data represents.\nThe first function converts a \u0026lsquo;list of lists\u0026rsquo; -\u0026gt; \u0026lsquo;one list\u0026rsquo;\ndef changeDataType(doc): '''Change Data Type into One List for POS''' final_doc = [] for sentence in doc: str1 = ' '.join(sentence) nlp_doc = nlp(str1) final_doc.append(nlp_doc) return final_doc doc_one_list = changeDataType(doc_lemmatization)  The second function returns each word with their POS tag.\ndef pos_tagging(doc): '''POS Tagging on Text Data''' doc_pos = [] for sentence in doc: for word in sentence: print (word, word.tag_, word.pos_, spacy.explain(word.tag_)) doc_pos.append(word.pos_) return doc_pos doc_pos = pos_tagging(doc_one_list) doc_pos  Matcher\nSpaCy also offers functionality to match on a pattern, here I have set to match on the words:\n Tom Jerry  # Import spaCy Matcher from spacy.matcher import Matcher # Initialize the matcher with the spaCy vocabulary matcher = Matcher(nlp.vocab) matches = [] for i in doc_lemmatization: print(i) str2 = ' '.join(i) doc = nlp(str2) # Define rule pattern = [{'TEXT': 'Tom'}, {'TEXT': 'Jerry'}] # Add rule matcher.add('rule_1', None, pattern) matches.append(matcher) matches_found = matcher(doc) matches.append(matches_found)  # Finding matches and passing the doc to the matches object matches  You can see from the image below, we have been provided with a unique id, and the respective start element 6 and end element 8 of the match Sentiment Analysis\nWe now create a score to see how positive or negative each cat fact is, by looping through the sentences and applying sentiment.polarity on each element within the list.\nscore_list = [] for i in doc_lemmatization: str3 = ' '.join(i) blob = TextBlob(str3) for sentence in blob.sentences: score_list.append(sentence.sentiment.polarity) print(i,sentence.sentiment.polarity)  You can see the text after NLP next to the score where:\n 1 is the most positive cat fact -1 is the most negative cat fact  df = pd.DataFrame(score_list) df['score'] = df[0] del df[0] df df['Sentiment'] = np.where(df['score'] == 0, 'Neutral', '') df['Neutral Flag'] = np.where(df['score'] == 0, 1, 0) df['Sentiment'] = np.where(df['score'] \u0026gt; 0, 'Positive', df['Sentiment']) df['Positive Flag'] = np.where(df['score'] \u0026gt; 0, 1, 0) df['Sentiment'] = np.where(df['score'] \u0026lt; 0, 'Negative', df['Sentiment']) df['Negative Flag'] = np.where(df['score'] \u0026lt; 0, 1, 0) df['Sentiment'] df = df[:20] df  Classifying the data into Positive, Negative and Neutral depending on the score. Data Viz\nBeing a big fan of Data Viz, what a better way to end with some graphs. I am a big fan of Plotly, but you can adapt the code for your preferred tools.\nPie Chart\nfig = px.pie(df, names='Sentiment', title='Sentiment Analysis of Random Cat Facts') fig.update_traces(textposition='inside', textinfo='percent+label') fig.show()  Histogram\ncf.go_offline() cf.set_config_file(offline=False, world_readable=True) df['score'].iplot( kind='hist', bins=50, xTitle='polarity', linecolor='black', yTitle='count', title='Sentiment Polarity Distribution')  Bar Chart Of Length Of Sentences\nlen_sentence = [] for i in doc_lemmatization: len_sentence.append(len(i)) =df['len_sentence'] = len_sentence  df['len_sentence'].iplot( kind='hist', bins=100, xTitle='word count', linecolor='black', yTitle='count', title='Review Text Word Count Distribution')  WordCloud\nmerged = list(itertools.chain(*doc_lemmatization)) #convert list to string and generate unique_string=(\u0026quot; \u0026quot;).join(merged) wordcloud = WordCloud(width = 1000, height = 500).generate(unique_string) plt.figure(figsize=(15,8)) plt.imshow(wordcloud) plt.axis(\u0026quot;off\u0026quot;) plt.show() plt.close()  Bar Chart Of POS Tagging\ncounts = Counter(doc_pos) common = counts.most_common() df_pos = pd.DataFrame(common) df_pos['Count'] = df_pos[1] df_pos['POS_tagging'] = df_pos[0] fig = px.bar(df_pos, x='POS_tagging', y='Count', title='Bar Chart of POS Tagging') fig.show()  Conclusion\nThat\u0026rsquo;s it for today. I hope you enjoyed this project as much as I have. Whilst there were limitations in the analysis due to the small sample size used, I believe the three things you can take from the project are:\n Introduction to APIs Introduction to NLP Introduction to Sentiment Analysis  If you have time, you can further explore this project by potentially using the dates and creating a time series based model looking to predict the average sentiment score.\n","date":1606089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606089600,"objectID":"f36c70a2fe5bf29e43a4a687477e6176","permalink":"https://shaanaucharagram.com/post/cat-facts-project/","publishdate":"2020-11-23T00:00:00Z","relpermalink":"/post/cat-facts-project/","section":"post","summary":"Natural Language Processing using SpaCy and Classifying Random Cat Facts Retrieved Via a Web API Into Positive, Negative or Neutral Using Sentiment Analysis.","tags":["Python","NLP","Sentiment Analysis"],"title":"Classifying Random Cat Facts using NLP and Sentiment Analysis","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"adfde6528264cf02951cd0222680f518","permalink":"https://shaanaucharagram.com/project/genzoomer-ai/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/genzoomer-ai/","section":"project","summary":"A global platform dedicated to offering young individuals a voice through articles without the jargon.","tags":["Website"],"title":"GenZoomer.ai","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"36040fded7332d3cad0addaff1ee8f32","permalink":"https://shaanaucharagram.com/project/personal_website/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/personal_website/","section":"project","summary":"My website, the site you are on right now, made in R using the blogdown package.","tags":["Website"],"title":"My Personal Website","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"b6d11a0e834a90e5f7086edae0c6360d","permalink":"https://shaanaucharagram.com/project/wing-it/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/wing-it/","section":"project","summary":"My sixth form coursework, made in C# with Unity based on Flappy Bird but with additional functionality.","tags":["Other"],"title":"Recreating Flappy Bird With a Twist","type":"project"},{"authors":["Shaan Aucharagram"],"categories":["Machine Learning"],"content":"Hello everyone, over the past couple of years you have probably heard about the new phase within the industry regarding Artificial Intelligence. As a result, many companies have gone on to hire data specialists \u0026amp; machine learning engineers without truly understanding what they provide and whether their company has the right infrastructure set up. In light of this, I will be talking about some of the key concepts in Machine Learning which is a subset of artificial intelligence, and is the “Field of programming which gives computers the ability to learn without being explicitly programmed.” Arthur Samuel\nWhat does that mean exactly? Well from a very high-level (easy-to-understand) perspective it means that data is input into a model created by data scientists, which from the existing data will use the data given to the computer and make a prediction. A good example is predicting stocks, you provide data of existing stock information for the program to make a prediction of stock prices.\nThere are three main types of Machine Learning that I wish to talk about within this post, where I will go into a little bit of detail regarding how they work, for people unfamiliar with these concepts and for those wishing to refresh their memory.\nReinforcement Learning\nReinforcement learning is commonly used within fields such as Robotics \u0026amp; Game Theory and where there is a multitude of actions that can occur, given an initial state. What this means, is that the machine will learn by itself, then optimize from the previous choices made to take the next best action. This is different from the other types of machine learning which may not output the “optimised solution”. An example of this would be if you were to have a Flappy Bird game programmed by reinforcement learning, the first attempt of the bird may initially collide to the ground, then it may collide with a pole, however, reinforcement learning will eventually optimise the birds route after each failed attempt until it creates the best path for the bird, to get as far as possible in the game. This could be applied to other types of games as well, for example, an AI from a chess game, can be programmed using reinforcement learning, and from each game played it will gradually become better over time. Reinforcement learning is generally not used within the industry apart from the fields mentioned above, mainly because companies prefer to use other methods which are generally easier to program, however noticeable exceptions that are becoming increasingly popular within the industry are Chatbots and devices such as Alexa and Siri, these are examples of reinforcement learning.\nSupervised Machine Learning\nSupervised Machine Learning, is the second of the three fields within Machine Learning and by far the most popular field used within the industry. This is because Supervised Machine Learning is trained on labelled data and there is already a rough idea of what the prediction will be. For example, what mode of transport will Person A take to work? Or what is the value of Company A? Thus, before the algorithm is implemented, there is already a rough idea of what algorithm will predict. Hence, the program knows there is a value/category which the outcome can be picked from. One of the most common types of Supervised Machine Learning examples within the Data Science community is the Titanic: Machine Learning problem, where you use Classification to predict whether each passenger survives or doesn’t. I will now try and break it down further.\nThere are two main types of Supervised Machine Learning Algorithms:\n Regression Classification  Classification is used in problems such as predicting what mode of transport Person A takes to work, so the training data (data which the model uses to learn from) is based on categorical data. Hence, as the classes of transport are already defined, i.e. bus, car, train etc. the question is a classification problem.\nWhereas Regression is used in problems which still have a rough idea of what value will be predicted, however, the input data is continuous and an example would be predicting the weight of Person A or the value of Company A. Hence, regression is used when you have a continuous output.\nThis can be further broken down however, for now, I will leave it at that and I hope to break it down further in a later post.\nUnsupervised Machine Learning\nFinally, the last type of Machine Learning, where you have your training data but no idea what the prediction will be based on the input data. You may be thinking, that sounds awfully similar to Reinforcement Learning? However, reinforcement learning is focused on taking the next action, i.e. simulating the next move within a Chess game. Whereas, Unsupervised Learning is more focused on finding the patterns and differences between sets of data. An example of this would be a 3D Cluster with the respective axis of age, salary and gender and thus, create its own categories for data points which have similar results on the 3D cluster.\nUnsupervised Learning is different from supervised due to assigning what data points it believes would be categories rather than having pre-defined categories such as mode of transport for Person A to work.\nUnsupervised learning will generally require more “computational” power than supervised learning, and supervised learning is generally more accurate than unsupervised learning. So where possible, it is advised to use supervised learning over unsupervised learning.\nI will delve into more detail about unsupervised learning in a different post, as I wish to keep this article about the basics, and there is so much more to explore.\nThank you for taking the time to read my “What Is Machine Learning.” I hope you found it useful, regardless of your industry \u0026amp; sector of work/education and now you won’t need to nod your head when a colleague mentions Artificial Intelligence or Machine Learning without knowing what it truly achieves within the workplace.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"4ce8b9f9543d3b94e91a5ea960d97d03","permalink":"https://shaanaucharagram.com/post/machine-learning-post/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/machine-learning-post/","section":"post","summary":"Machine Learning explained in an easy-to-understand format without the jargon.","tags":["Machine Learning","Artificial Intelligence"],"title":"What Is Machine Learning?","type":"post"}]